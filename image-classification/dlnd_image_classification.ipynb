{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [00:13, 12.6MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/cifar/cifar-10-python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 2:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 984, 1: 1007, 2: 1010, 3: 995, 4: 1010, 5: 988, 6: 1008, 7: 1026, 8: 987, 9: 985}\n",
      "First 20 Labels: [1, 6, 6, 8, 8, 3, 4, 6, 0, 6, 0, 3, 6, 6, 5, 4, 8, 3, 2, 6]\n",
      "\n",
      "Example of Image 3:\n",
      "Image - Min Value: 4 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 8 Name: ship\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9ADQ2P8FrDreNOinmWwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe9902c3198>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 2\n",
    "sample_id = 3\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1 (initially from 0 to 255).\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    norm_data = np.array(x)\n",
    "    return (255 - norm_data) / 255\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # Create the encoder object using the same mapping for labels [0,9], regardless of parameter x.\n",
    "    encoder = LabelBinarizer()\n",
    "    encoder.fit(np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n",
    "    \n",
    "    # Create np array from list\n",
    "    labels_array = np.array(x)\n",
    "        \n",
    "    # Apply One-Hot Encoding    \n",
    "    labels_array = encoder.transform(labels_array)\n",
    "    \n",
    "    # Change to float32, so it can be multiplied against the features in TensorFlow, which are float32\n",
    "    labels_array = labels_array.astype(np.float32)\n",
    "    \n",
    "    return labels_array\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    shape_param = (None,) + image_shape\n",
    "    return tf.placeholder(dtype=tf.float32, shape=shape_param, name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(dtype=tf.float32, shape=(None, n_classes), name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(dtype=tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # weight needs the last dimension (i.e, the # of channels in the images) of input tensor.\n",
    "    # The *tuple syntax expands the tuple elements into arguments.\n",
    "    weight = tf.Variable(tf.truncated_normal([*conv_ksize, x_tensor.get_shape().as_list()[-1], conv_num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.truncated_normal([conv_num_outputs], stddev=0.1))\n",
    "    conv = tf.nn.conv2d(x_tensor, weight, strides=[1, *conv_strides, 1], padding='SAME')\n",
    "    conv = tf.nn.bias_add(conv, bias)\n",
    "    tf.nn.relu(conv)\n",
    "    return tf.nn.max_pool(conv, ksize=[1, *pool_ksize, 1], strides=[1, *pool_strides, 1], padding='SAME')\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    flattened_size = 1\n",
    "    # Multiply all the tensor dimensions together, except the batch size, which is the position zero in shape tuple\n",
    "    for dim in x_tensor.get_shape()[1:].as_list():\n",
    "        flattened_size *= dim\n",
    "    return tf.reshape(x_tensor, [-1, flattened_size])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    flattened_size = x_tensor.get_shape().as_list()[-1]\n",
    "    \n",
    "    weight = tf.Variable(tf.truncated_normal([flattened_size, num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.truncated_normal([num_outputs], stddev=0.1))\n",
    "    con = tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "    return tf.nn.relu(con)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    weight = tf.Variable(tf.truncated_normal([x_tensor.get_shape().as_list()[-1], num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.truncated_normal([num_outputs], stddev=0.1))\n",
    "    return tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Input shape: (?, 32, 32, 3)\n",
      "Convolution Layer 1 with Max Pooling shape: (?, 16, 16, 32)\n",
      "Convolution Layer 2 with Max Pooling shape: (?, 8, 8, 64)\n",
      "Convolution Layer 3 with Max Pooling shape: (?, 4, 4, 128)\n",
      "Flatten Layer shape: (?, 2048)\n",
      "Fully Connected Layer 1 shape: (?, 2048)\n",
      "Fully Connected Layer 2 shape: (?, 1024)\n",
      "Output shape: (?, 10)\n",
      "\n",
      "\n",
      "Input shape: (?, 32, 32, 3)\n",
      "Convolution Layer 1 with Max Pooling shape: (?, 16, 16, 32)\n",
      "Convolution Layer 2 with Max Pooling shape: (?, 8, 8, 64)\n",
      "Convolution Layer 3 with Max Pooling shape: (?, 4, 4, 128)\n",
      "Flatten Layer shape: (?, 2048)\n",
      "Fully Connected Layer 1 shape: (?, 2048)\n",
      "Fully Connected Layer 2 shape: (?, 1024)\n",
      "Output shape: (?, 10)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    print('\\n\\nInput shape: {}'.format(x.get_shape()))\n",
    "    conv1 = conv2d_maxpool(x, conv_num_outputs = 32, conv_ksize = (2, 2), conv_strides = (1, 1), pool_ksize = (2,2), pool_strides = (2, 2)) \n",
    "    print('Convolution Layer 1 with Max Pooling shape: {}'.format(conv1.get_shape()))\n",
    "    conv2 = conv2d_maxpool(conv1, conv_num_outputs = 64, conv_ksize = (2, 2), conv_strides = (1, 1), pool_ksize = (2,2), pool_strides = (2, 2))\n",
    "    print('Convolution Layer 2 with Max Pooling shape: {}'.format(conv2.get_shape()))\n",
    "    conv3 = conv2d_maxpool(conv2, conv_num_outputs = 128, conv_ksize = (2, 2), conv_strides = (1, 1), pool_ksize = (2,2), pool_strides = (2, 2))\n",
    "    print('Convolution Layer 3 with Max Pooling shape: {}'.format(conv3.get_shape()))\n",
    "    \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flat = flatten(conv3)\n",
    "    print('Flatten Layer shape: {}'.format(flat.get_shape()))\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    con1 = fully_conn(flat, 2048)\n",
    "    con1 = tf.nn.dropout(con1, keep_prob)\n",
    "    print('Fully Connected Layer 1 shape: {}'.format(con1.get_shape()))\n",
    "    con2 = fully_conn(con1, 1024)\n",
    "    con2 = tf.nn.dropout(con2, keep_prob)\n",
    "    print('Fully Connected Layer 2 shape: {}'.format(con2.get_shape()))\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    out = output(con2, 10)\n",
    "    print('Output shape: {}'.format(out.get_shape()))\n",
    "    \n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict={\n",
    "                x: feature_batch,\n",
    "                y: label_batch,\n",
    "                keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # Calculate batch loss and accuracy\n",
    "    loss = session.run(cost, feed_dict={\n",
    "                x: feature_batch,\n",
    "                y: label_batch,\n",
    "                keep_prob: 1.0})\n",
    "    valid_acc = sess.run(accuracy, feed_dict={\n",
    "                x: valid_features,\n",
    "                y: valid_labels,\n",
    "                keep_prob: 1.0})\n",
    "    print('Loss: {} Validation Accuracy: {}'.format(loss, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "# We have chosen a fairly complex model architecture, so let's use a low keep probability to avoid overfitting.\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 1.817765474319458 Validation Accuracy: 0.38940000534057617\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 1.6224507093429565 Validation Accuracy: 0.4365999698638916\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 1.3827660083770752 Validation Accuracy: 0.476999968290329\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 1.1682106256484985 Validation Accuracy: 0.5053999423980713\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 1.0929120779037476 Validation Accuracy: 0.5175999402999878\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 0.9136113524436951 Validation Accuracy: 0.5537999272346497\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 0.8106734752655029 Validation Accuracy: 0.562999963760376\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 0.6690276861190796 Validation Accuracy: 0.5879998803138733\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 0.5604748725891113 Validation Accuracy: 0.5893998742103577\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 0.48709118366241455 Validation Accuracy: 0.5913999080657959\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 0.4105476140975952 Validation Accuracy: 0.5897998809814453\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 0.3357699513435364 Validation Accuracy: 0.6121998429298401\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 0.27009546756744385 Validation Accuracy: 0.6173999309539795\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 0.21990181505680084 Validation Accuracy: 0.6017998456954956\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 0.12152102589607239 Validation Accuracy: 0.621199905872345\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 0.09415731579065323 Validation Accuracy: 0.6283999085426331\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 0.1019100770354271 Validation Accuracy: 0.6111999154090881\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 0.0753699243068695 Validation Accuracy: 0.6151999235153198\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 0.055600278079509735 Validation Accuracy: 0.6317998766899109\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 0.04956614226102829 Validation Accuracy: 0.6057999134063721\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 0.026925906538963318 Validation Accuracy: 0.6191999316215515\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 0.01701674982905388 Validation Accuracy: 0.6317998766899109\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 0.013537606224417686 Validation Accuracy: 0.6309999227523804\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 0.014850642532110214 Validation Accuracy: 0.6201999187469482\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 0.015217306092381477 Validation Accuracy: 0.6213999390602112\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 0.009946079924702644 Validation Accuracy: 0.6317998766899109\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 0.007429192308336496 Validation Accuracy: 0.6161998510360718\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 0.003542451187968254 Validation Accuracy: 0.6261998414993286\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 0.00200770515948534 Validation Accuracy: 0.6235998868942261\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 0.0036814389750361443 Validation Accuracy: 0.6187999248504639\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 0.0009456642437726259 Validation Accuracy: 0.6353998780250549\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 0.0011646398343145847 Validation Accuracy: 0.6281998753547668\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 0.002244077157229185 Validation Accuracy: 0.63319993019104\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 0.0014446867862716317 Validation Accuracy: 0.6321998834609985\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 0.0012759522069245577 Validation Accuracy: 0.6345999240875244\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 0.00037994328886270523 Validation Accuracy: 0.6303998827934265\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 0.0018843578873202205 Validation Accuracy: 0.6185999512672424\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 0.0027434041257947683 Validation Accuracy: 0.6189998388290405\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 0.0007153758779168129 Validation Accuracy: 0.6239998936653137\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 0.00046841136645525694 Validation Accuracy: 0.6271998882293701\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 0.0006337929517030716 Validation Accuracy: 0.6245998740196228\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 0.0008372655138373375 Validation Accuracy: 0.6405999064445496\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 0.0004544347757473588 Validation Accuracy: 0.6297999024391174\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 0.00018207677931059152 Validation Accuracy: 0.6377999186515808\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 0.0003212798619642854 Validation Accuracy: 0.6323999166488647\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 0.0002196048735640943 Validation Accuracy: 0.6325998902320862\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 0.00016030407277867198 Validation Accuracy: 0.6237999200820923\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 0.004637523554265499 Validation Accuracy: 0.6149998903274536\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 0.0002685686049517244 Validation Accuracy: 0.6249998807907104\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 0.0006917502032592893 Validation Accuracy: 0.6345999240875244\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 0.0003891506348736584 Validation Accuracy: 0.6207998991012573\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 0.00018409700714983046 Validation Accuracy: 0.6201999187469482\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 0.003582400269806385 Validation Accuracy: 0.6019999384880066\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 0.00031627307180315256 Validation Accuracy: 0.6287998557090759\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 0.00021548008953686804 Validation Accuracy: 0.6347998976707458\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 0.000212875209399499 Validation Accuracy: 0.6201999187469482\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 0.0009038193384185433 Validation Accuracy: 0.6101999282836914\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 0.0005443543777801096 Validation Accuracy: 0.6203998923301697\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 8.77048441907391e-05 Validation Accuracy: 0.6329998970031738\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 6.74761031405069e-05 Validation Accuracy: 0.6141998767852783\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 5.946651072008535e-05 Validation Accuracy: 0.6339998841285706\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.0003294936032034457 Validation Accuracy: 0.6137999296188354\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.0002152033121092245 Validation Accuracy: 0.6351999044418335\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.0004585985152516514 Validation Accuracy: 0.6373999118804932\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.0011699023889377713 Validation Accuracy: 0.6223999261856079\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.00070286012487486 Validation Accuracy: 0.6159998774528503\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.0001024060693453066 Validation Accuracy: 0.6285998821258545\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 7.604974234709516e-05 Validation Accuracy: 0.6353999376296997\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 3.740298780030571e-05 Validation Accuracy: 0.6327999234199524\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 2.917665187851526e-05 Validation Accuracy: 0.6307998895645142\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.0001278696727240458 Validation Accuracy: 0.6315999031066895\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.0003327085287310183 Validation Accuracy: 0.6129999160766602\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 0.00013457935710903257 Validation Accuracy: 0.614599883556366\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.0008920836262404919 Validation Accuracy: 0.6093999743461609\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.0001157366277766414 Validation Accuracy: 0.6251999139785767\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 0.0006383976433426142 Validation Accuracy: 0.6187999248504639\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.00011317045573377982 Validation Accuracy: 0.622999906539917\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 0.00013061566278338432 Validation Accuracy: 0.6275999546051025\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 5.1326343964319676e-05 Validation Accuracy: 0.6269999146461487\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 0.000420458207372576 Validation Accuracy: 0.619999885559082\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 2.6133551727980375e-05 Validation Accuracy: 0.6279999613761902\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 6.178980402182788e-05 Validation Accuracy: 0.6283999681472778\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 4.076821824128274e-06 Validation Accuracy: 0.6211999654769897\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 5.552326183533296e-05 Validation Accuracy: 0.6241999268531799\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.0010485646780580282 Validation Accuracy: 0.6277998685836792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 2.3485168640036136e-05 Validation Accuracy: 0.626599907875061\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 3.6098113923799247e-05 Validation Accuracy: 0.6297999620437622\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 1.3979021787235979e-05 Validation Accuracy: 0.6393998861312866\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.0017319321632385254 Validation Accuracy: 0.6291999220848083\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 6.751008186256513e-05 Validation Accuracy: 0.6311998963356018\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 6.198682513058884e-06 Validation Accuracy: 0.6273999214172363\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 1.5747795259812847e-05 Validation Accuracy: 0.6365999579429626\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 2.774536824290408e-06 Validation Accuracy: 0.6339999437332153\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 2.202358018621453e-06 Validation Accuracy: 0.6311999559402466\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 8.368553244508803e-05 Validation Accuracy: 0.6311999559402466\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 7.021135388640687e-06 Validation Accuracy: 0.6341999173164368\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 5.814175437990343e-06 Validation Accuracy: 0.6239999532699585\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.00011590604844968766 Validation Accuracy: 0.6309998631477356\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 1.1005199667124543e-05 Validation Accuracy: 0.6221998929977417\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 1.2367867157081491e-06 Validation Accuracy: 0.6267998814582825\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 1.8097198009490967 Validation Accuracy: 0.3895999789237976\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss: 1.6145873069763184 Validation Accuracy: 0.45419996976852417\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss: 1.4202250242233276 Validation Accuracy: 0.4793999195098877\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss: 1.3890835046768188 Validation Accuracy: 0.5115999579429626\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss: 1.5024363994598389 Validation Accuracy: 0.5133999586105347\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 1.3187342882156372 Validation Accuracy: 0.515999972820282\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss: 1.2528396844863892 Validation Accuracy: 0.5311999320983887\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss: 1.108533501625061 Validation Accuracy: 0.5615999102592468\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss: 1.060787320137024 Validation Accuracy: 0.582599937915802\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss: 1.1207518577575684 Validation Accuracy: 0.5823999643325806\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 1.0711954832077026 Validation Accuracy: 0.595599889755249\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss: 0.9633067846298218 Validation Accuracy: 0.5747998952865601\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss: 0.9364439845085144 Validation Accuracy: 0.5935999155044556\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss: 0.8983635306358337 Validation Accuracy: 0.6139999628067017\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss: 0.9930607080459595 Validation Accuracy: 0.6307999491691589\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 0.9224197864532471 Validation Accuracy: 0.6261999011039734\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss: 0.7381411790847778 Validation Accuracy: 0.6339998841285706\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss: 0.7452329397201538 Validation Accuracy: 0.6351999640464783\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss: 0.7597770094871521 Validation Accuracy: 0.6385998725891113\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss: 0.7498283386230469 Validation Accuracy: 0.6585999131202698\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 0.722980797290802 Validation Accuracy: 0.6547998189926147\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss: 0.6808032393455505 Validation Accuracy: 0.6563998460769653\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss: 0.6930838227272034 Validation Accuracy: 0.6285998821258545\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss: 0.6255240440368652 Validation Accuracy: 0.6609998345375061\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss: 0.6507977843284607 Validation Accuracy: 0.6765998601913452\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 0.568742036819458 Validation Accuracy: 0.6543998718261719\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss: 0.46486493945121765 Validation Accuracy: 0.6871998310089111\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss: 0.4742387533187866 Validation Accuracy: 0.6789999008178711\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss: 0.4527045786380768 Validation Accuracy: 0.6859998106956482\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss: 0.48992982506752014 Validation Accuracy: 0.6937998533248901\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 0.5434753894805908 Validation Accuracy: 0.6651999354362488\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss: 0.3731597661972046 Validation Accuracy: 0.6785999536514282\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss: 0.3072139024734497 Validation Accuracy: 0.7017998695373535\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss: 0.41204655170440674 Validation Accuracy: 0.7033998966217041\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss: 0.43231892585754395 Validation Accuracy: 0.6961998343467712\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 0.3894532322883606 Validation Accuracy: 0.6921998858451843\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss: 0.2510794997215271 Validation Accuracy: 0.7003998160362244\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss: 0.26562392711639404 Validation Accuracy: 0.727199912071228\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss: 0.3508690595626831 Validation Accuracy: 0.7043998837471008\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss: 0.29541417956352234 Validation Accuracy: 0.7113999128341675\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 0.33449020981788635 Validation Accuracy: 0.7113999128341675\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss: 0.2128586769104004 Validation Accuracy: 0.7101998925209045\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss: 0.1529395580291748 Validation Accuracy: 0.7235997915267944\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss: 0.22203418612480164 Validation Accuracy: 0.7257997989654541\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss: 0.19798226654529572 Validation Accuracy: 0.7269998788833618\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 0.2620468735694885 Validation Accuracy: 0.715799868106842\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss: 0.15774068236351013 Validation Accuracy: 0.7165998220443726\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss: 0.16112421452999115 Validation Accuracy: 0.7255998253822327\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss: 0.16947105526924133 Validation Accuracy: 0.7345998287200928\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss: 0.20455682277679443 Validation Accuracy: 0.7309998273849487\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 0.18404622375965118 Validation Accuracy: 0.7025998830795288\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss: 0.11170756816864014 Validation Accuracy: 0.7149998545646667\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss: 0.14200901985168457 Validation Accuracy: 0.715199887752533\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss: 0.1385478675365448 Validation Accuracy: 0.7155998945236206\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss: 0.13468250632286072 Validation Accuracy: 0.7199999094009399\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 0.17313189804553986 Validation Accuracy: 0.7265998721122742\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss: 0.08384328335523605 Validation Accuracy: 0.7279998660087585\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss: 0.08005647361278534 Validation Accuracy: 0.7213999032974243\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss: 0.1279275119304657 Validation Accuracy: 0.7279998660087585\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss: 0.09014853835105896 Validation Accuracy: 0.7423998117446899\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 0.15141254663467407 Validation Accuracy: 0.7367998957633972\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss: 0.06827864050865173 Validation Accuracy: 0.7389998435974121\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss: 0.05270496755838394 Validation Accuracy: 0.7279998064041138\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss: 0.09258615970611572 Validation Accuracy: 0.7285997867584229\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss: 0.0832386165857315 Validation Accuracy: 0.7301998138427734\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 0.09333492070436478 Validation Accuracy: 0.7357998490333557\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss: 0.04745445027947426 Validation Accuracy: 0.731799840927124\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss: 0.0848340392112732 Validation Accuracy: 0.734799861907959\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss: 0.06647186726331711 Validation Accuracy: 0.7289998531341553\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss: 0.057669155299663544 Validation Accuracy: 0.7303998470306396\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 0.06981769949197769 Validation Accuracy: 0.7189998626708984\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss: 0.048460703343153 Validation Accuracy: 0.7343997955322266\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss: 0.032498642802238464 Validation Accuracy: 0.7311998605728149\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss: 0.0336296483874321 Validation Accuracy: 0.7357997894287109\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss: 0.03564435616135597 Validation Accuracy: 0.7379997968673706\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 0.033783502876758575 Validation Accuracy: 0.7481998205184937\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss: 0.044381171464920044 Validation Accuracy: 0.7379997968673706\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss: 0.013349175453186035 Validation Accuracy: 0.7399997711181641\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss: 0.04025007784366608 Validation Accuracy: 0.7307997941970825\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss: 0.017621731385588646 Validation Accuracy: 0.7471998929977417\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 0.031343281269073486 Validation Accuracy: 0.7403998374938965\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss: 0.008159949444234371 Validation Accuracy: 0.745999813079834\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss: 0.02053951285779476 Validation Accuracy: 0.7369998097419739\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss: 0.019374001771211624 Validation Accuracy: 0.7305998206138611\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss: 0.031030697748064995 Validation Accuracy: 0.7427998781204224\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 0.029399456456303596 Validation Accuracy: 0.7411998510360718\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss: 0.006676109042018652 Validation Accuracy: 0.734799861907959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, CIFAR-10 Batch 3:  Loss: 0.01904168911278248 Validation Accuracy: 0.7379997968673706\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss: 0.02251700684428215 Validation Accuracy: 0.742999792098999\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss: 0.024609975516796112 Validation Accuracy: 0.7379998564720154\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 0.01963241770863533 Validation Accuracy: 0.7307997941970825\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss: 0.009521178901195526 Validation Accuracy: 0.7409998178482056\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss: 0.016819924116134644 Validation Accuracy: 0.7371998429298401\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss: 0.022591078653931618 Validation Accuracy: 0.7345998287200928\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss: 0.010678199119865894 Validation Accuracy: 0.7421998381614685\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 0.012848841026425362 Validation Accuracy: 0.7459998726844788\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss: 0.01163574680685997 Validation Accuracy: 0.7437998652458191\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss: 0.019334867596626282 Validation Accuracy: 0.7429999113082886\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss: 0.010549159720540047 Validation Accuracy: 0.7463998794555664\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss: 0.00560862198472023 Validation Accuracy: 0.7371997833251953\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 0.020531676709651947 Validation Accuracy: 0.7417998313903809\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss: 0.006974413059651852 Validation Accuracy: 0.7337997555732727\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss: 0.005981652531772852 Validation Accuracy: 0.7385997772216797\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss: 0.015439891256392002 Validation Accuracy: 0.7467998266220093\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss: 0.00596641143783927 Validation Accuracy: 0.7411998510360718\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 0.0063804155215620995 Validation Accuracy: 0.746799886226654\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss: 0.0065162223763763905 Validation Accuracy: 0.7321999073028564\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss: 0.018921559676527977 Validation Accuracy: 0.7421998381614685\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss: 0.012760312296450138 Validation Accuracy: 0.738399863243103\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss: 0.003416188061237335 Validation Accuracy: 0.7357998490333557\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 0.009829480201005936 Validation Accuracy: 0.7293998003005981\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss: 0.0033223815262317657 Validation Accuracy: 0.7475998401641846\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss: 0.007450527045875788 Validation Accuracy: 0.7365998029708862\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss: 0.011595457792282104 Validation Accuracy: 0.7449998259544373\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss: 0.005279995501041412 Validation Accuracy: 0.7475998401641846\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 0.009702556766569614 Validation Accuracy: 0.7369998693466187\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss: 0.002924500033259392 Validation Accuracy: 0.738399863243103\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss: 0.003605710808187723 Validation Accuracy: 0.7441998720169067\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss: 0.008132010698318481 Validation Accuracy: 0.7363998889923096\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss: 0.004048577044159174 Validation Accuracy: 0.7409998774528503\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 0.0016448711976408958 Validation Accuracy: 0.7341998815536499\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss: 0.0035801006015390158 Validation Accuracy: 0.7257998585700989\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss: 0.005012640729546547 Validation Accuracy: 0.7381998300552368\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss: 0.005409471224993467 Validation Accuracy: 0.7321999073028564\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss: 0.00945521704852581 Validation Accuracy: 0.7389999032020569\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 0.006233025807887316 Validation Accuracy: 0.7395998239517212\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss: 0.0025976737961173058 Validation Accuracy: 0.7435998916625977\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss: 0.004808548837900162 Validation Accuracy: 0.739599883556366\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss: 0.013376899063587189 Validation Accuracy: 0.7359998226165771\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss: 0.0037658221554011106 Validation Accuracy: 0.7379998564720154\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 0.0044942498207092285 Validation Accuracy: 0.7271998524665833\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss: 0.0042703417129814625 Validation Accuracy: 0.7341998219490051\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss: 0.0027405135333538055 Validation Accuracy: 0.7391998767852783\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss: 0.009422213770449162 Validation Accuracy: 0.7427998781204224\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss: 0.0057656425051391125 Validation Accuracy: 0.7367998957633972\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 0.011573372408747673 Validation Accuracy: 0.738399863243103\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss: 0.001437226077541709 Validation Accuracy: 0.7431998252868652\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss: 0.002533325459808111 Validation Accuracy: 0.7341998815536499\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss: 0.0038861942011862993 Validation Accuracy: 0.7471998333930969\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss: 0.0223418939858675 Validation Accuracy: 0.7297998070716858\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 0.0021910564973950386 Validation Accuracy: 0.7465998530387878\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss: 0.009798543527722359 Validation Accuracy: 0.7451997995376587\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss: 0.0007625330472365022 Validation Accuracy: 0.7487998008728027\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss: 0.0031573406886309385 Validation Accuracy: 0.7389999032020569\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss: 0.003868974046781659 Validation Accuracy: 0.7311998605728149\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 0.0013156639179214835 Validation Accuracy: 0.7377998232841492\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss: 0.0013554820325225592 Validation Accuracy: 0.7399998307228088\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss: 0.0007723767776042223 Validation Accuracy: 0.7391998767852783\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss: 0.0015017742989584804 Validation Accuracy: 0.7357997894287109\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss: 0.0031405731569975615 Validation Accuracy: 0.7481998205184937\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 0.0017821758519858122 Validation Accuracy: 0.7387998700141907\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss: 0.006146269850432873 Validation Accuracy: 0.7405998706817627\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss: 0.0011645848862826824 Validation Accuracy: 0.752599835395813\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss: 0.003283735830336809 Validation Accuracy: 0.7423998117446899\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss: 0.004105073399841785 Validation Accuracy: 0.7417998313903809\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 0.0025985294487327337 Validation Accuracy: 0.7223998308181763\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss: 0.001462309854105115 Validation Accuracy: 0.7433998584747314\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss: 0.0015997779555618763 Validation Accuracy: 0.75139981508255\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss: 0.002919302089139819 Validation Accuracy: 0.744399905204773\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss: 0.003517656121402979 Validation Accuracy: 0.7441998720169067\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 0.007838280871510506 Validation Accuracy: 0.7325998544692993\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss: 0.000827853858936578 Validation Accuracy: 0.7179998755455017\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss: 0.0016073009464889765 Validation Accuracy: 0.7455999255180359\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss: 0.009461691603064537 Validation Accuracy: 0.741399884223938\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss: 0.0010211662156507373 Validation Accuracy: 0.7449998259544373\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 0.0007859962643124163 Validation Accuracy: 0.7299998998641968\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss: 0.00180183001793921 Validation Accuracy: 0.7369998693466187\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss: 0.000708335021045059 Validation Accuracy: 0.7411998510360718\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss: 0.004399995785206556 Validation Accuracy: 0.7373998761177063\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss: 0.001878555165603757 Validation Accuracy: 0.7427998781204224\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 0.0024229774717241526 Validation Accuracy: 0.7345998287200928\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss: 0.0015063778264448047 Validation Accuracy: 0.737799882888794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, CIFAR-10 Batch 3:  Loss: 0.0013325188774615526 Validation Accuracy: 0.7483998537063599\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss: 0.0023335397709161043 Validation Accuracy: 0.7439998388290405\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss: 0.0005619017174467444 Validation Accuracy: 0.7321998476982117\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 0.0013255031080916524 Validation Accuracy: 0.7423998117446899\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss: 0.0008532180218026042 Validation Accuracy: 0.7463998794555664\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss: 0.000884196488186717 Validation Accuracy: 0.7467998266220093\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss: 0.003883791621774435 Validation Accuracy: 0.7421997785568237\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss: 0.0031593255698680878 Validation Accuracy: 0.7451998591423035\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 0.0012650906573981047 Validation Accuracy: 0.7393998503684998\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss: 0.001408754731528461 Validation Accuracy: 0.7399998903274536\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss: 0.00032625827589072287 Validation Accuracy: 0.7427998781204224\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss: 0.0024118521250784397 Validation Accuracy: 0.7311998009681702\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss: 0.0007212861673906446 Validation Accuracy: 0.744799792766571\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 0.0005064134020358324 Validation Accuracy: 0.7409998178482056\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss: 0.00015143763448577374 Validation Accuracy: 0.7451998591423035\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss: 0.00038255820982158184 Validation Accuracy: 0.7379998564720154\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss: 0.0013504771050065756 Validation Accuracy: 0.7465998530387878\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss: 0.0011260752798989415 Validation Accuracy: 0.7451997995376587\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 0.0008396474877372384 Validation Accuracy: 0.7289998531341553\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss: 0.0003401836729608476 Validation Accuracy: 0.7393998503684998\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss: 0.001499899080954492 Validation Accuracy: 0.7399998903274536\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss: 0.0017398446798324585 Validation Accuracy: 0.741199791431427\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss: 0.0013385580386966467 Validation Accuracy: 0.7381998300552368\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 0.0009114307467825711 Validation Accuracy: 0.7433998584747314\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss: 0.0005934785585850477 Validation Accuracy: 0.7351998686790466\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss: 0.0037040957249701023 Validation Accuracy: 0.7459999322891235\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss: 0.0010288964258506894 Validation Accuracy: 0.7425998449325562\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss: 0.0026271208189427853 Validation Accuracy: 0.7463998794555664\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 0.0008236884605139494 Validation Accuracy: 0.7345998883247375\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss: 0.0001575039204908535 Validation Accuracy: 0.7489998936653137\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss: 0.0021077790297567844 Validation Accuracy: 0.7437998652458191\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss: 0.003370377467945218 Validation Accuracy: 0.7463998794555664\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss: 0.0062745059840381145 Validation Accuracy: 0.7431998252868652\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 0.0002864861744455993 Validation Accuracy: 0.7529999017715454\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss: 7.522133819293231e-05 Validation Accuracy: 0.7387998104095459\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss: 0.0008544048178009689 Validation Accuracy: 0.7443997859954834\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss: 0.0018133152043446898 Validation Accuracy: 0.7387998700141907\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss: 0.0004758416907861829 Validation Accuracy: 0.7461998462677002\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 0.0009370071347802877 Validation Accuracy: 0.7341998815536499\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss: 0.00036416074726730585 Validation Accuracy: 0.7341998219490051\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss: 0.0022192862816154957 Validation Accuracy: 0.7425998449325562\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss: 0.0026245799381285906 Validation Accuracy: 0.7471998333930969\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss: 0.0015912167727947235 Validation Accuracy: 0.7379997968673706\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 0.0013459093170240521 Validation Accuracy: 0.7391998171806335\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss: 0.0005667094956152141 Validation Accuracy: 0.7435998320579529\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss: 0.004615608137100935 Validation Accuracy: 0.7331998348236084\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss: 0.0003931709215976298 Validation Accuracy: 0.7415999174118042\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss: 0.0008145495667122304 Validation Accuracy: 0.7449998259544373\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 0.0009861531434580684 Validation Accuracy: 0.7429998517036438\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss: 7.243030268000439e-05 Validation Accuracy: 0.7471998333930969\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss: 0.00016212000628001988 Validation Accuracy: 0.7447998523712158\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss: 0.0024400048423558474 Validation Accuracy: 0.7421998977661133\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss: 0.0006479867734014988 Validation Accuracy: 0.7397998571395874\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 0.0005020504468120635 Validation Accuracy: 0.7447998523712158\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss: 8.070268086157739e-05 Validation Accuracy: 0.7395998239517212\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss: 0.003050921019166708 Validation Accuracy: 0.7471998929977417\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss: 0.005592805799096823 Validation Accuracy: 0.7291998863220215\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss: 0.0005616219714283943 Validation Accuracy: 0.7471998333930969\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 0.00018615501176100224 Validation Accuracy: 0.7385998368263245\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss: 0.001224221778102219 Validation Accuracy: 0.7409998178482056\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss: 0.003347279503941536 Validation Accuracy: 0.7387998700141907\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss: 0.01166372001171112 Validation Accuracy: 0.7397998571395874\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss: 0.0009496420971117914 Validation Accuracy: 0.744399905204773\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 0.0002377150667598471 Validation Accuracy: 0.7387997508049011\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss: 0.000252585276030004 Validation Accuracy: 0.744399905204773\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss: 0.00033873296342790127 Validation Accuracy: 0.7451998591423035\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss: 0.0022053346037864685 Validation Accuracy: 0.7403998374938965\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss: 0.00025062329950742424 Validation Accuracy: 0.7503998279571533\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 0.0004794770502485335 Validation Accuracy: 0.7437998652458191\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss: 0.0006798022659495473 Validation Accuracy: 0.7469998598098755\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss: 0.00017154500528704375 Validation Accuracy: 0.7497997879981995\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss: 0.00108244433067739 Validation Accuracy: 0.7409997582435608\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss: 0.0016204703133553267 Validation Accuracy: 0.7435998320579529\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 0.00094373500905931 Validation Accuracy: 0.7363998293876648\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss: 0.00042623389163054526 Validation Accuracy: 0.7409998178482056\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss: 7.36045913072303e-05 Validation Accuracy: 0.7493998408317566\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss: 0.0008318989421240985 Validation Accuracy: 0.7437998652458191\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss: 0.0010323834139853716 Validation Accuracy: 0.7323998212814331\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 0.00038263070746324956 Validation Accuracy: 0.7469998598098755\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss: 0.003857778385281563 Validation Accuracy: 0.7387998700141907\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss: 0.00013286124158184975 Validation Accuracy: 0.7407998442649841\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss: 0.0007866158848628402 Validation Accuracy: 0.7431998252868652\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss: 0.00029528571758419275 Validation Accuracy: 0.736599862575531\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 0.0008661028114147484 Validation Accuracy: 0.745999813079834\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss: 0.003970901016145945 Validation Accuracy: 0.7419998645782471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52, CIFAR-10 Batch 3:  Loss: 7.287239714059979e-05 Validation Accuracy: 0.7439998984336853\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss: 0.0008369893184863031 Validation Accuracy: 0.740199863910675\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss: 0.0012002014555037022 Validation Accuracy: 0.7417998313903809\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 0.001387156662531197 Validation Accuracy: 0.7455998063087463\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss: 0.0004496907058637589 Validation Accuracy: 0.7377998232841492\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss: 0.00017345146625302732 Validation Accuracy: 0.7389998435974121\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss: 0.0003577956813387573 Validation Accuracy: 0.7421997785568237\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss: 0.0001952541060745716 Validation Accuracy: 0.7467998266220093\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 0.0013272329233586788 Validation Accuracy: 0.7493999004364014\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss: 0.00021624029614031315 Validation Accuracy: 0.7409998774528503\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss: 0.000217221153434366 Validation Accuracy: 0.7495998740196228\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss: 0.00018963703769259155 Validation Accuracy: 0.7425997853279114\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss: 0.008059226907789707 Validation Accuracy: 0.7379998564720154\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 0.0011217400897294283 Validation Accuracy: 0.7389997839927673\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss: 0.0001377422158839181 Validation Accuracy: 0.7465997934341431\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss: 5.748515832237899e-05 Validation Accuracy: 0.7433998584747314\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss: 0.00012563675409182906 Validation Accuracy: 0.7541998028755188\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss: 0.0006532379193231463 Validation Accuracy: 0.7409998178482056\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 0.0002326226094737649 Validation Accuracy: 0.747799813747406\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss: 0.000665368337649852 Validation Accuracy: 0.7343998551368713\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss: 0.00010244748409604654 Validation Accuracy: 0.7373998761177063\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss: 0.0003459380241110921 Validation Accuracy: 0.731799840927124\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss: 0.0023953288327902555 Validation Accuracy: 0.7385998368263245\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 0.0008835290791466832 Validation Accuracy: 0.7403998970985413\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss: 0.00025106515386141837 Validation Accuracy: 0.7509998679161072\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss: 0.00010078865307150409 Validation Accuracy: 0.7469998598098755\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss: 0.00012201294885016978 Validation Accuracy: 0.7427998185157776\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss: 0.0008095735101960599 Validation Accuracy: 0.7391998171806335\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 0.002051174407824874 Validation Accuracy: 0.7431998252868652\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss: 0.0005308862891979516 Validation Accuracy: 0.7453998327255249\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss: 0.001300065079703927 Validation Accuracy: 0.7397998571395874\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss: 0.00025458456366322935 Validation Accuracy: 0.7273998260498047\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss: 0.00010723866580519825 Validation Accuracy: 0.7471998929977417\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 0.0004200623370707035 Validation Accuracy: 0.7411998510360718\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss: 4.292470111977309e-05 Validation Accuracy: 0.7427998185157776\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss: 0.0007959376089274883 Validation Accuracy: 0.7373998165130615\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss: 4.434284346643835e-05 Validation Accuracy: 0.7367998957633972\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss: 7.677904068259522e-05 Validation Accuracy: 0.7435998320579529\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 0.0004385735956020653 Validation Accuracy: 0.7413997650146484\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss: 3.0323162718559615e-05 Validation Accuracy: 0.7423998117446899\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss: 0.00010665520676411688 Validation Accuracy: 0.7373998165130615\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss: 0.0003594673762563616 Validation Accuracy: 0.7521998286247253\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss: 0.00032546056900173426 Validation Accuracy: 0.7433998584747314\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 0.00016219787357840687 Validation Accuracy: 0.7451999187469482\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss: 0.0005675515276379883 Validation Accuracy: 0.741399884223938\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss: 0.00042337903869338334 Validation Accuracy: 0.7399998903274536\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss: 9.094180859392509e-05 Validation Accuracy: 0.7403998374938965\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss: 0.0006985839572735131 Validation Accuracy: 0.7425997853279114\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.0003779593389481306 Validation Accuracy: 0.7441998720169067\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss: 0.0006660130457021296 Validation Accuracy: 0.7391998171806335\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss: 0.00033029148471541703 Validation Accuracy: 0.7389998435974121\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss: 0.013610935769975185 Validation Accuracy: 0.7489998936653137\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss: 0.003231104463338852 Validation Accuracy: 0.7349998354911804\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.0010229689069092274 Validation Accuracy: 0.7279999256134033\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss: 0.0008038877858780324 Validation Accuracy: 0.751599907875061\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss: 0.00028282596031203866 Validation Accuracy: 0.7421997785568237\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss: 0.001598696457222104 Validation Accuracy: 0.7397998571395874\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss: 0.0007382470066659153 Validation Accuracy: 0.7473998069763184\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.0001241702848346904 Validation Accuracy: 0.7361998558044434\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss: 0.00021510581427719444 Validation Accuracy: 0.7373998761177063\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss: 9.394210792379454e-05 Validation Accuracy: 0.7341998219490051\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss: 0.02668164111673832 Validation Accuracy: 0.7403998374938965\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss: 0.00029721000464633107 Validation Accuracy: 0.7431998252868652\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 9.719825902720913e-05 Validation Accuracy: 0.734799861907959\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss: 8.588356286054477e-05 Validation Accuracy: 0.7361998558044434\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss: 1.8339087546337396e-05 Validation Accuracy: 0.7343997955322266\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss: 0.0007511878502555192 Validation Accuracy: 0.7357998490333557\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss: 6.83233593008481e-05 Validation Accuracy: 0.7481998205184937\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 6.641640356974676e-05 Validation Accuracy: 0.7423999309539795\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss: 0.00010427740926388651 Validation Accuracy: 0.7391998171806335\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss: 5.9989084547851235e-05 Validation Accuracy: 0.746199905872345\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss: 0.00030734261963516474 Validation Accuracy: 0.7441998720169067\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss: 0.0032410311978310347 Validation Accuracy: 0.7403998970985413\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 7.287529297173023e-05 Validation Accuracy: 0.7523999214172363\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss: 0.0001133003388531506 Validation Accuracy: 0.7435998320579529\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss: 0.0010326402261853218 Validation Accuracy: 0.734799861907959\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss: 0.0027748998254537582 Validation Accuracy: 0.7447998523712158\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss: 0.00012262501695659012 Validation Accuracy: 0.7469998002052307\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 1.892180080176331e-05 Validation Accuracy: 0.7505998611450195\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss: 0.00035025092074647546 Validation Accuracy: 0.7381998300552368\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss: 7.566479325760156e-06 Validation Accuracy: 0.7415997982025146\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss: 0.00028619388467632234 Validation Accuracy: 0.7467998266220093\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss: 0.00014385665417648852 Validation Accuracy: 0.7485997676849365\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 7.935816938697826e-06 Validation Accuracy: 0.7497997879981995\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss: 0.0003710359160322696 Validation Accuracy: 0.7361998558044434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69, CIFAR-10 Batch 3:  Loss: 3.4314041840843856e-05 Validation Accuracy: 0.7433997988700867\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss: 0.00027853454230353236 Validation Accuracy: 0.7351998686790466\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss: 0.0004965830012224615 Validation Accuracy: 0.7445998191833496\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 4.268658813089132e-05 Validation Accuracy: 0.7419998645782471\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss: 4.899420673609711e-05 Validation Accuracy: 0.7463997602462769\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss: 0.00044680459541268647 Validation Accuracy: 0.7409998774528503\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss: 6.766998558305204e-05 Validation Accuracy: 0.7393999099731445\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss: 8.122921281028539e-05 Validation Accuracy: 0.7453998327255249\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.0005418601795099676 Validation Accuracy: 0.7349998950958252\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss: 0.0008319651824422181 Validation Accuracy: 0.744799792766571\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss: 0.0006786836893297732 Validation Accuracy: 0.7407999038696289\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss: 0.0010340349981561303 Validation Accuracy: 0.7543997764587402\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss: 6.366675370372832e-05 Validation Accuracy: 0.7429998517036438\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.0007875817245803773 Validation Accuracy: 0.737799882888794\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss: 1.9806402633548714e-05 Validation Accuracy: 0.7485998272895813\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss: 0.000741576193831861 Validation Accuracy: 0.7451999187469482\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss: 0.0003261748352088034 Validation Accuracy: 0.7487998008728027\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss: 0.00019560445798560977 Validation Accuracy: 0.7445998191833496\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 7.470633136108518e-05 Validation Accuracy: 0.7473998069763184\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss: 8.239605085691437e-05 Validation Accuracy: 0.7469998598098755\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss: 2.7535839763004333e-05 Validation Accuracy: 0.7527998089790344\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss: 0.00047523004468530416 Validation Accuracy: 0.7423997521400452\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss: 0.0004362113540992141 Validation Accuracy: 0.7407999038696289\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.00021853840735275298 Validation Accuracy: 0.7327998280525208\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss: 0.00017340478370897472 Validation Accuracy: 0.7465997934341431\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss: 3.184409433742985e-05 Validation Accuracy: 0.738399863243103\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss: 9.520359890302643e-05 Validation Accuracy: 0.7437998056411743\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss: 0.0010063688969239593 Validation Accuracy: 0.7453998327255249\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.001790867536328733 Validation Accuracy: 0.7441998720169067\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss: 0.00039307778934016824 Validation Accuracy: 0.7357998490333557\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss: 2.3856453481130302e-05 Validation Accuracy: 0.7427998781204224\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss: 0.00024249643320217729 Validation Accuracy: 0.7481998801231384\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss: 0.0034050587564706802 Validation Accuracy: 0.7437998056411743\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 5.9214675275143236e-05 Validation Accuracy: 0.7471998929977417\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss: 0.0006015935214236379 Validation Accuracy: 0.7511998414993286\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss: 0.00029240897856652737 Validation Accuracy: 0.745999813079834\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss: 0.0002961026912089437 Validation Accuracy: 0.7421997785568237\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss: 0.0024640262126922607 Validation Accuracy: 0.7497998476028442\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.00011638701107585803 Validation Accuracy: 0.7407999038696289\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss: 0.0007205946603789926 Validation Accuracy: 0.7369998693466187\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss: 0.00033658448955975473 Validation Accuracy: 0.7461997866630554\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss: 2.6321002223994583e-05 Validation Accuracy: 0.7397998571395874\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss: 0.00017521253903396428 Validation Accuracy: 0.7345998287200928\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 7.906094833742827e-05 Validation Accuracy: 0.7413998246192932\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss: 0.00010827001096913591 Validation Accuracy: 0.7425997853279114\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss: 6.379435944836587e-05 Validation Accuracy: 0.7447998523712158\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss: 0.001917085493914783 Validation Accuracy: 0.7371998429298401\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss: 7.451559940818697e-05 Validation Accuracy: 0.749799907207489\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 0.0005474669160321355 Validation Accuracy: 0.7465998530387878\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss: 0.00014207638741936535 Validation Accuracy: 0.7451998591423035\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss: 8.275119034806266e-05 Validation Accuracy: 0.7399998307228088\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss: 0.0009320424869656563 Validation Accuracy: 0.7429998517036438\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss: 0.000499904272146523 Validation Accuracy: 0.7373998165130615\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 6.744737038388848e-05 Validation Accuracy: 0.7321998476982117\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss: 0.00016010800027288496 Validation Accuracy: 0.7411998510360718\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss: 5.88867033002316e-06 Validation Accuracy: 0.7515998482704163\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss: 0.000926606182474643 Validation Accuracy: 0.7497997879981995\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss: 2.9338405511225574e-05 Validation Accuracy: 0.7419998645782471\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 2.5392620955244638e-05 Validation Accuracy: 0.7421998977661133\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss: 6.552439299412072e-05 Validation Accuracy: 0.7413997650146484\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss: 7.382664625765756e-05 Validation Accuracy: 0.7407998442649841\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss: 0.00011553527292562649 Validation Accuracy: 0.7481998205184937\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss: 5.7952056522481143e-05 Validation Accuracy: 0.7423998117446899\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 0.00013732477964367718 Validation Accuracy: 0.7463998198509216\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss: 0.0001711548975436017 Validation Accuracy: 0.7413997650146484\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss: 8.099169644992799e-05 Validation Accuracy: 0.7425998449325562\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss: 0.00030013654031790793 Validation Accuracy: 0.7437998056411743\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss: 0.0006891734665259719 Validation Accuracy: 0.7353998422622681\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 0.0001923166128108278 Validation Accuracy: 0.7411998510360718\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss: 7.73167994339019e-05 Validation Accuracy: 0.7355998158454895\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss: 0.0008920953841879964 Validation Accuracy: 0.7295998334884644\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss: 0.00046920403838157654 Validation Accuracy: 0.7487998604774475\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss: 0.00014928591554053128 Validation Accuracy: 0.7411998510360718\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 0.0005211555981077254 Validation Accuracy: 0.7461998462677002\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss: 1.7429199942853302e-05 Validation Accuracy: 0.7435998320579529\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss: 2.4883280275389552e-05 Validation Accuracy: 0.7413998246192932\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss: 4.268849443178624e-05 Validation Accuracy: 0.7451997995376587\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss: 0.00023160332057159394 Validation Accuracy: 0.7501998543739319\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.003429675241932273 Validation Accuracy: 0.7391997575759888\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss: 5.629147926811129e-05 Validation Accuracy: 0.7399998307228088\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss: 0.00023154111113399267 Validation Accuracy: 0.7429998517036438\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss: 0.00010723888408392668 Validation Accuracy: 0.741199791431427\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss: 6.83618854964152e-05 Validation Accuracy: 0.7461997866630554\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 0.00012150131806265563 Validation Accuracy: 0.7503997683525085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86, CIFAR-10 Batch 2:  Loss: 2.025683716055937e-05 Validation Accuracy: 0.7495998740196228\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss: 1.340425296803005e-05 Validation Accuracy: 0.7463998794555664\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss: 0.00010772526729851961 Validation Accuracy: 0.7501997947692871\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss: 5.259738099994138e-05 Validation Accuracy: 0.7377998232841492\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 0.0005114173982292414 Validation Accuracy: 0.7469998002052307\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss: 0.00011862941755680367 Validation Accuracy: 0.747799813747406\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss: 1.3365415725274943e-05 Validation Accuracy: 0.7421998977661133\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss: 2.4675869099155534e-06 Validation Accuracy: 0.7485998272895813\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss: 1.2136932127759792e-05 Validation Accuracy: 0.741199791431427\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.0007389129023067653 Validation Accuracy: 0.7469998598098755\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss: 5.676976797985844e-06 Validation Accuracy: 0.7405998110771179\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss: 1.5545028873020783e-05 Validation Accuracy: 0.7409998774528503\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss: 1.8084223484038375e-05 Validation Accuracy: 0.7375999093055725\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss: 9.694459004094824e-05 Validation Accuracy: 0.7447998523712158\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.0001520375080872327 Validation Accuracy: 0.744399905204773\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss: 0.0004843085480388254 Validation Accuracy: 0.741199791431427\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss: 1.963101385626942e-05 Validation Accuracy: 0.7497997879981995\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss: 7.590444147353992e-05 Validation Accuracy: 0.7465998530387878\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss: 1.237785181729123e-05 Validation Accuracy: 0.7399997711181641\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 3.620471034082584e-05 Validation Accuracy: 0.7453997731208801\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss: 9.17939905775711e-05 Validation Accuracy: 0.7417998313903809\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss: 8.311046258313581e-05 Validation Accuracy: 0.7487999200820923\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss: 0.0001494252501288429 Validation Accuracy: 0.7339998483657837\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss: 3.202175139449537e-05 Validation Accuracy: 0.7453998923301697\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.004506517667323351 Validation Accuracy: 0.7415997982025146\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss: 0.00027954450342804193 Validation Accuracy: 0.7461997866630554\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss: 2.5331773940706626e-06 Validation Accuracy: 0.738399863243103\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss: 5.113471706863493e-05 Validation Accuracy: 0.7515997886657715\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss: 7.857204036554322e-05 Validation Accuracy: 0.7405998706817627\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 1.3076589311822318e-05 Validation Accuracy: 0.7395998239517212\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss: 2.054328433587216e-05 Validation Accuracy: 0.7441998720169067\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss: 0.0006413761875592172 Validation Accuracy: 0.7347998023033142\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss: 8.213345427066088e-05 Validation Accuracy: 0.7403997778892517\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss: 6.687482709821779e-06 Validation Accuracy: 0.7407998442649841\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 8.063113637035713e-05 Validation Accuracy: 0.7527998685836792\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss: 8.472217450616881e-05 Validation Accuracy: 0.7437998056411743\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss: 7.36287038307637e-05 Validation Accuracy: 0.7485998868942261\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss: 1.009353218250908e-05 Validation Accuracy: 0.7383999228477478\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss: 1.8576656657387502e-05 Validation Accuracy: 0.7405998110771179\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.00039875757647678256 Validation Accuracy: 0.7381998300552368\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss: 4.875598824582994e-05 Validation Accuracy: 0.744999885559082\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss: 0.00014037253276910633 Validation Accuracy: 0.7393999099731445\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss: 7.377201109193265e-05 Validation Accuracy: 0.7433998584747314\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss: 1.7515380022814497e-05 Validation Accuracy: 0.7409998774528503\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.0008296085288748145 Validation Accuracy: 0.7433997988700867\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss: 5.816368502564728e-05 Validation Accuracy: 0.7463998198509216\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss: 1.4115561498329043e-05 Validation Accuracy: 0.7465997934341431\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss: 0.00023835587489884347 Validation Accuracy: 0.744799792766571\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss: 3.489228038233705e-05 Validation Accuracy: 0.7443998456001282\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.0006533713894896209 Validation Accuracy: 0.7461998462677002\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss: 0.00039089962956495583 Validation Accuracy: 0.7369998693466187\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss: 0.00016882624186109751 Validation Accuracy: 0.7397997975349426\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss: 2.4438133550575003e-05 Validation Accuracy: 0.7379998564720154\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss: 3.868287876684917e-06 Validation Accuracy: 0.7419998645782471\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.0005163875175639987 Validation Accuracy: 0.7455998063087463\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss: 0.0004433724097907543 Validation Accuracy: 0.7441998720169067\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss: 0.0027074464596807957 Validation Accuracy: 0.7475997805595398\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss: 0.002562949201092124 Validation Accuracy: 0.7373998165130615\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss: 5.957226676400751e-06 Validation Accuracy: 0.7515998482704163\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.0002139960415661335 Validation Accuracy: 0.7381998896598816\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss: 0.0006867626798339188 Validation Accuracy: 0.7423998117446899\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss: 1.3399449926509988e-05 Validation Accuracy: 0.7401998043060303\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss: 0.00012156068260082975 Validation Accuracy: 0.7415997982025146\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss: 0.00563842011615634 Validation Accuracy: 0.738399863243103\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.0010330292861908674 Validation Accuracy: 0.7427998185157776\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss: 2.866662180167623e-05 Validation Accuracy: 0.7415997982025146\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss: 6.6660813899943605e-06 Validation Accuracy: 0.7393998503684998\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss: 5.626379788736813e-06 Validation Accuracy: 0.7421998977661133\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss: 1.2884765965281986e-05 Validation Accuracy: 0.7433998584747314\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 6.461873272201046e-05 Validation Accuracy: 0.7353997826576233\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss: 4.4460713979788125e-05 Validation Accuracy: 0.7385998964309692\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss: 3.2603222734906012e-06 Validation Accuracy: 0.7435998320579529\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss: 9.986202712752856e-06 Validation Accuracy: 0.7463998794555664\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss: 5.966319440631196e-05 Validation Accuracy: 0.7435998320579529\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.7305181962025317\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArsAAAJ/CAYAAAB4GhsgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSG5psisiIiIiufX/AR0CpdkyJwcfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f71243f7e80>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 349
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
